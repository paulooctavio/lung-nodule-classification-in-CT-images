{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4276813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d5b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa3939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0619fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4bbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3be0138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1875                       Loss D: 0.7153, loss G: 0.7063\n",
      "Epoch [1/50] Batch 0/1875                       Loss D: 0.1910, loss G: 1.9701\n",
      "Epoch [2/50] Batch 0/1875                       Loss D: 0.5620, loss G: 0.9624\n",
      "Epoch [3/50] Batch 0/1875                       Loss D: 0.4107, loss G: 1.2220\n",
      "Epoch [4/50] Batch 0/1875                       Loss D: 0.7106, loss G: 0.9478\n",
      "Epoch [5/50] Batch 0/1875                       Loss D: 0.8144, loss G: 0.8264\n",
      "Epoch [6/50] Batch 0/1875                       Loss D: 0.8269, loss G: 0.8230\n",
      "Epoch [7/50] Batch 0/1875                       Loss D: 0.5531, loss G: 1.1744\n",
      "Epoch [8/50] Batch 0/1875                       Loss D: 0.3424, loss G: 1.5049\n",
      "Epoch [9/50] Batch 0/1875                       Loss D: 0.8202, loss G: 0.7040\n",
      "Epoch [10/50] Batch 0/1875                       Loss D: 0.4011, loss G: 1.3633\n",
      "Epoch [11/50] Batch 0/1875                       Loss D: 0.6523, loss G: 1.3840\n",
      "Epoch [12/50] Batch 0/1875                       Loss D: 0.7117, loss G: 1.1971\n",
      "Epoch [13/50] Batch 0/1875                       Loss D: 0.4300, loss G: 1.4132\n",
      "Epoch [14/50] Batch 0/1875                       Loss D: 0.5292, loss G: 1.4079\n",
      "Epoch [15/50] Batch 0/1875                       Loss D: 0.4472, loss G: 1.3601\n",
      "Epoch [16/50] Batch 0/1875                       Loss D: 0.5141, loss G: 1.2720\n",
      "Epoch [17/50] Batch 0/1875                       Loss D: 0.6120, loss G: 1.2046\n",
      "Epoch [18/50] Batch 0/1875                       Loss D: 0.7325, loss G: 0.8979\n",
      "Epoch [19/50] Batch 0/1875                       Loss D: 0.6862, loss G: 0.9084\n",
      "Epoch [20/50] Batch 0/1875                       Loss D: 0.7677, loss G: 1.0634\n",
      "Epoch [21/50] Batch 0/1875                       Loss D: 0.4491, loss G: 1.4571\n",
      "Epoch [22/50] Batch 0/1875                       Loss D: 0.7046, loss G: 1.0043\n",
      "Epoch [23/50] Batch 0/1875                       Loss D: 0.5062, loss G: 1.2712\n",
      "Epoch [24/50] Batch 0/1875                       Loss D: 0.5235, loss G: 1.4084\n",
      "Epoch [25/50] Batch 0/1875                       Loss D: 0.7412, loss G: 1.0021\n",
      "Epoch [26/50] Batch 0/1875                       Loss D: 0.4926, loss G: 1.3056\n",
      "Epoch [27/50] Batch 0/1875                       Loss D: 0.6155, loss G: 1.5365\n",
      "Epoch [28/50] Batch 0/1875                       Loss D: 0.6167, loss G: 0.9781\n",
      "Epoch [29/50] Batch 0/1875                       Loss D: 0.4130, loss G: 2.3526\n",
      "Epoch [30/50] Batch 0/1875                       Loss D: 0.6381, loss G: 1.0828\n",
      "Epoch [31/50] Batch 0/1875                       Loss D: 0.8025, loss G: 1.0860\n",
      "Epoch [32/50] Batch 0/1875                       Loss D: 0.6658, loss G: 1.2677\n",
      "Epoch [33/50] Batch 0/1875                       Loss D: 0.5659, loss G: 1.2650\n",
      "Epoch [34/50] Batch 0/1875                       Loss D: 0.5645, loss G: 1.2981\n",
      "Epoch [35/50] Batch 0/1875                       Loss D: 0.6962, loss G: 1.5948\n",
      "Epoch [36/50] Batch 0/1875                       Loss D: 0.7116, loss G: 1.0791\n",
      "Epoch [37/50] Batch 0/1875                       Loss D: 0.4588, loss G: 1.5713\n",
      "Epoch [38/50] Batch 0/1875                       Loss D: 0.4389, loss G: 1.4479\n",
      "Epoch [39/50] Batch 0/1875                       Loss D: 0.5729, loss G: 1.4511\n",
      "Epoch [40/50] Batch 0/1875                       Loss D: 0.6209, loss G: 1.1308\n",
      "Epoch [41/50] Batch 0/1875                       Loss D: 0.5277, loss G: 1.1269\n",
      "Epoch [42/50] Batch 0/1875                       Loss D: 0.7302, loss G: 1.0388\n",
      "Epoch [43/50] Batch 0/1875                       Loss D: 0.5119, loss G: 1.4349\n",
      "Epoch [44/50] Batch 0/1875                       Loss D: 0.5861, loss G: 1.2851\n",
      "Epoch [45/50] Batch 0/1875                       Loss D: 0.4997, loss G: 1.0781\n",
      "Epoch [46/50] Batch 0/1875                       Loss D: 0.5651, loss G: 1.1494\n",
      "Epoch [47/50] Batch 0/1875                       Loss D: 0.5463, loss G: 1.3628\n",
      "Epoch [48/50] Batch 0/1875                       Loss D: 0.5558, loss G: 1.3585\n",
      "Epoch [49/50] Batch 0/1875                       Loss D: 0.7199, loss G: 0.9053\n"
     ]
    }
   ],
   "source": [
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30337875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59278e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
