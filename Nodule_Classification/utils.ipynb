{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nrrd\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "import pydicom as dicomio\n",
    "\n",
    "from pydicom import dcmread\n",
    "from scipy import ndimage\n",
    "from skimage.util import montage\n",
    "from skimage.measure import label, regionprops, find_contours\n",
    "from skimage.morphology import disk, binary_dilation, binary_opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32578265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "EXAMS_DIR = \"/Storage/PauloOctavioDir/Exames/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f03da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    min_val = MIN_HU_VALUE\n",
    "    max_val = MAX_HU_VALUE\n",
    "    image[image < min_val] = min_val\n",
    "    image[image > max_val] = max_val\n",
    "    image = (image - min_val) / (max_val - min_val)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "274806d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_hu(scans):\n",
    "    image = np.stack([s.pixel_array for s in scans])\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "    \n",
    "    # Convert to Hounsfield units (HU)\n",
    "    intercept = -1024\n",
    "    slope = 1\n",
    "        \n",
    "    image += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80979cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scans in given folder path\n",
    "def load_exam(path):\n",
    "    slices = [dcmread(path + '/' + s) for s in os.listdir(path) if s != 'rtss.dcm']\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    volume = get_pixels_hu(slices)  \n",
    "    return volume\n",
    "\n",
    "def load_mask(path):\n",
    "    slices = [dcmread(path + '/' + s) for s in os.listdir(path) if s != 'rtss.dcm']\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    volume = np.stack([s.pixel_array for s in slices])\n",
    "    return np.array(volume, dtype=np.int16)\n",
    "\n",
    "def get_volumes(images_path, masks_path, exam_id):\n",
    "    root_path = EXAMS_DIR + str(exam_id)\n",
    "    exam = load_exam(root_path + images_path)\n",
    "    masks = load_mask(root_path + masks_path)\n",
    "    return exam, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138751a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_montage(volume, cmap=None, div=10):\n",
    "    no_cols = np.ceil(volume.shape[0] / div).astype(int)\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    ax1.imshow(montage(volume, grid_shape=(div,no_cols)), cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7172b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/41793931/plotting-images-side-by-side-using-matplotlib\n",
    "def show_image_list(list_images, list_titles=None, list_cmaps=None, grid=False, num_cols=2, figsize=(8, 4), title_fontsize=15,\n",
    "                   hide_axis=False, save_fig=False, vmin=None, vmax=None, output_path=None):\n",
    "    '''\n",
    "    Shows a grid of images, where each image is a Numpy array. The images can be either\n",
    "    RGB or grayscale.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    images: list\n",
    "        List of the images to be displayed.\n",
    "    list_titles: list or None\n",
    "        Optional list of titles to be shown for each image.\n",
    "    list_cmaps: list or None\n",
    "        Optional list of cmap values for each image. If None, then cmap will be\n",
    "        automatically inferred.\n",
    "    grid: boolean\n",
    "        If True, show a grid over each image\n",
    "    num_cols: int\n",
    "        Number of columns to show.\n",
    "    figsize: tuple of width, height\n",
    "        Value to be passed to pyplot.figure()\n",
    "    title_fontsize: int\n",
    "        Value to be passed to set_title().\n",
    "    hide_axis: bool\n",
    "        If True, hide images axis.\n",
    "    save_fig: bool\n",
    "        If True, saves image list.\n",
    "    vmin, vmax : scalar, optional\n",
    "        When using scalar data and no explicit *norm*, *vmin* and *vmax*\n",
    "        define the data range that the colormap covers. By default,\n",
    "        the colormap covers the complete value range of the supplied\n",
    "        data. *vmin*, *vmax* are ignored if the *norm* parameter is used.\n",
    "    output_path: str\n",
    "        Value to be passed to pyplot.savefig()\n",
    "    '''\n",
    "\n",
    "    assert isinstance(list_images, list)\n",
    "    assert len(list_images) > 0\n",
    "    assert isinstance(list_images[0], np.ndarray)\n",
    "\n",
    "    if list_titles is not None:\n",
    "        assert isinstance(list_titles, list)\n",
    "        assert len(list_images) == len(list_titles), '%d imgs != %d titles' % (len(list_images), len(list_titles))\n",
    "\n",
    "    if list_cmaps is not None:\n",
    "        assert isinstance(list_cmaps, list)\n",
    "        assert len(list_images) == len(list_cmaps), '%d imgs != %d cmaps' % (len(list_images), len(list_cmaps))\n",
    "\n",
    "    num_images  = len(list_images)\n",
    "    num_cols    = min(num_images, num_cols)\n",
    "    num_rows    = int(num_images / num_cols) + (1 if num_images % num_cols != 0 else 0)\n",
    "\n",
    "    # Create a grid of subplots.\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    \n",
    "    # Create list of axes for easy iteration.\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        list_axes = list(axes.flat)\n",
    "    else:\n",
    "        list_axes = [axes]\n",
    "\n",
    "    for i in range(num_images):\n",
    "\n",
    "        img    = list_images[i]\n",
    "        title  = list_titles[i] if list_titles is not None else None\n",
    "        cmap   = list_cmaps[i] if list_cmaps is not None else None\n",
    "        \n",
    "        list_axes[i].imshow(img, cmap=cmap, vmin=None, vmax=None)\n",
    "        list_axes[i].set_title(title, fontsize=title_fontsize) \n",
    "        list_axes[i].grid(grid)\n",
    "\n",
    "    for i in range(num_images, len(list_axes)):\n",
    "        list_axes[i].set_visible(False)\n",
    "    \n",
    "    if hide_axis:\n",
    "        for i in range(len(list_axes)):\n",
    "            list_axes[i].set_axis_off()\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if save_fig:\n",
    "        plt.savefig(output_path)\n",
    "    _ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d069e6",
   "metadata": {},
   "source": [
    "### Nodules RoI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292373cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(image, left, top, right, bottom):\n",
    "    croped_image = image[top:bottom,\n",
    "                left:right]\n",
    "    return croped_image\n",
    "\n",
    "def mask_to_bbox(mask):\n",
    "    # ref: https://github.com/nikhilroxtomar/Semantic-Segmentation-Mask-to-Bounding-Box/blob/main/mask_to_bbox.py\n",
    "    lbl = label(mask)\n",
    "    props = regionprops(lbl)\n",
    "    if len(props) == 0:\n",
    "        raise ValueError('No mask identified.')\n",
    "    if len(props) == 1:\n",
    "        prop = props[0]\n",
    "    else:\n",
    "        areas = [r.area for r in props]\n",
    "        areas.sort()\n",
    "        for region in props:\n",
    "            if region.area == areas[-1]:\n",
    "                prop = region\n",
    "    x1 = prop.bbox[1]\n",
    "    y1 = prop.bbox[0]\n",
    "    x2 = prop.bbox[3]\n",
    "    y2 = prop.bbox[2]\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    return bbox\n",
    "\n",
    "def mask_to_roi(mask, image, bbox_extension=0):\n",
    "    \n",
    "    bboxes = mask_to_bbox(mask)\n",
    "    cropped_image = crop_roi(\n",
    "        image, \n",
    "        bboxes[0] - bbox_extension, \n",
    "        bboxes[1] - bbox_extension, \n",
    "        bboxes[2] + bbox_extension, \n",
    "        bboxes[3] + bbox_extension\n",
    "    )\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "def get_pixel_array(dicom_file):\n",
    "    data = dcmread(dicom_file)\n",
    "    return data.pixel_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38b740",
   "metadata": {},
   "source": [
    "## DICOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "from pydicom.dataset import Dataset, FileDataset\n",
    "from pydicom.uid import ExplicitVRLittleEndian\n",
    "import pydicom._storage_sopclass_uids\n",
    "\n",
    "def write_dicom(image, filename, rescale_intercept=\"0\", rescale_slope=\"1\", pixel_spacing=r\"1\\1\"): \n",
    "    # ref: https://stackoverflow.com/questions/14350675/create-pydicom-file-from-numpy-array\n",
    "    if image.dtype != np.uint16:\n",
    "        image = image.astype(np.uint16)\n",
    "        \n",
    "    meta = pydicom.Dataset()\n",
    "    meta.MediaStorageSOPClassUID = pydicom._storage_sopclass_uids.CTImageStorage\n",
    "    meta.MediaStorageSOPInstanceUID = pydicom.uid.generate_uid()\n",
    "    meta.TransferSyntaxUID = pydicom.uid.ExplicitVRLittleEndian  \n",
    "\n",
    "    ds = Dataset()\n",
    "    ds.file_meta = meta\n",
    "\n",
    "    ds.is_little_endian = True\n",
    "    ds.is_implicit_VR = False\n",
    "\n",
    "    ds.SOPClassUID = pydicom._storage_sopclass_uids.CTImageStorage\n",
    "\n",
    "    ds.Modality = \"CT\"\n",
    "    ds.SeriesInstanceUID = pydicom.uid.generate_uid()\n",
    "    ds.StudyInstanceUID = pydicom.uid.generate_uid()\n",
    "    ds.FrameOfReferenceUID = pydicom.uid.generate_uid()\n",
    "\n",
    "    ds.BitsStored = 16\n",
    "    ds.BitsAllocated = 16\n",
    "    ds.SamplesPerPixel = 1\n",
    "    ds.HighBit = 15\n",
    "\n",
    "    ds.ImagesInAcquisition = \"1\"\n",
    "\n",
    "    ds.Rows = image.shape[0]\n",
    "    ds.Columns = image.shape[1]\n",
    "    ds.InstanceNumber = 1\n",
    "    \n",
    "    ds.RescaleIntercept = rescale_intercept\n",
    "    ds.RescaleSlope = rescale_slope\n",
    "    ds.PixelSpacing = pixel_spacing\n",
    "    ds.PhotometricInterpretation = \"MONOCHROME2\"\n",
    "    ds.PixelRepresentation = 1\n",
    "\n",
    "    pydicom.dataset.validate_file_meta(ds.file_meta, enforce_standard=True)\n",
    "        \n",
    "    ds.PixelData = image.tobytes()\n",
    "    ds.save_as(filename, write_like_original=False)\n",
    "    \n",
    "def show_dicom(dicom_file):\n",
    "    image = get_pixel_array(dicom_file)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e56b41",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n",
    "\n",
    "class WSConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight scaled Conv2d (Equalized Learning Rate)\n",
    "    Note that input is multiplied rather than changing weights\n",
    "    this will have the same result.\n",
    "\n",
    "    Inspired and looked at:\n",
    "    https://github.com/nvnbny/progressive_growing_of_gans/blob/master/modelUtils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # initial takes 1x1 -> 4x4\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(factors) - 1\n",
    "        ):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        # The number of channels in upscale will stay the same, while\n",
    "        # out which has moved through prog_blocks might change. To ensure\n",
    "        # we can convert both to rgb we use different rgb_layers\n",
    "        # (steps-1) and steps for upscaled, out respectively\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "    \n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8a29fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import png\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def denormalize(image):\n",
    "    min_val = MIN_HU_VALUE\n",
    "    max_val = MAX_HU_VALUE\n",
    "    return image * (max_val - min_val) + min_val\n",
    "\n",
    "def generate_images(gen, step, n, hist_type, path):\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            noise = torch.randn(1, Z_DIM, 1, 1).to(DEVICE)\n",
    "            fake_image = gen(noise, alpha, step)\n",
    "            fake_image = fake_image.cpu().numpy()\n",
    "            fake_image = fake_image * 0.5 + 0.5\n",
    "            fake_image = fake_image[0,0]\n",
    "            fake_image = denormalize(fake_image)\n",
    "            fake_image = np.around(fake_image)\n",
    "            image_file = f'GAN_{hist_type}_{str(i).zfill(4)}.dcm'\n",
    "            image_path = os.path.join(path, image_file)\n",
    "            write_dicom(fake_image, image_path)\n",
    "\n",
    "def dicom_to_png(dicom_file, output_dir, rescale_size=None):\n",
    "    data = dcmread(dicom_file)\n",
    "    image = data.pixel_array\n",
    "    intercept = int(data.RescaleIntercept)\n",
    "    slope = int(data.RescaleSlope)\n",
    "    image = slope * image + intercept\n",
    "    image = normalize(image)\n",
    "    image = image * 255\n",
    "    if rescale_size:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((rescale_size, rescale_size)),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "        image = np.array(image).astype('float32')\n",
    "        image = transform(image).numpy()[0]\n",
    "    \n",
    "    # Convert to uint\n",
    "    image = np.uint8(image)\n",
    "    shape = image.shape\n",
    "    # Write the PNG file\n",
    "    output_file = dicom_file.split('/')[-1].replace('dcm', 'png')\n",
    "    with open(output_dir + output_file, 'wb') as png_file:\n",
    "        w = png.Writer(shape[1], shape[0], greyscale=True)\n",
    "        w.write(png_file, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c0db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
