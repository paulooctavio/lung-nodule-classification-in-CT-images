{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e61ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pydicom import dcmread\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c876fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_tensorboard(\n",
    "    writer, loss_critic, loss_gen, real, fake, cur_res, lr, tensorboard_step\n",
    "):\n",
    "    writer.add_scalar(\"data/loss_dis\", loss_critic, global_step=tensorboard_step)\n",
    "    writer.add_scalar(\"data/loss_gen\", loss_gen, global_step=tensorboard_step)\n",
    "    writer.add_scalar(\"data/cur_resl\", int(cur_res), global_step=tensorboard_step)\n",
    "    writer.add_scalar(\"data/cur_lr\", lr, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # take out (up to) 8 examples to plot\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbf3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1856e",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa3fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 2e-4\n",
    "z_dim = 64\n",
    "image_dim = 64 * 64 * 1\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "TRAIN_DIR = f\"/Storage/PauloOctavioDir/nodule_images/images\"\n",
    "TENSORBOARD_MODEL_NAME = \"gan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e4ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(504),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0,), (4000,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f4710",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1a3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungNoduleDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        images = os.listdir(image_dir)\n",
    "        if \"rtss.dcm\" in images:\n",
    "            images.remove(\"rtss.dcm\")\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        data = dcmread(img_path)\n",
    "        image = np.array(data.pixel_array).astype(\"float32\")\n",
    "        # Conversio to HU\n",
    "        intercept = int(data.RescaleIntercept)\n",
    "        slope = int(data.RescaleSlope)\n",
    "        image = slope * image + intercept\n",
    "        image[image < -2000] = 0\n",
    "        image[image > 3000] = 3000\n",
    "        return self.transform(image)\n",
    "\n",
    "\n",
    "dataset = LungNoduleDataset(image_dir=TRAIN_DIR, transform=train_transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b1e699a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch 0/183                       Loss D: 0.6844, loss G: 0.8069\n",
      "Epoch [1/100] Batch 0/183                       Loss D: 0.0990, loss G: 3.8083\n",
      "Epoch [2/100] Batch 0/183                       Loss D: 0.8560, loss G: 1.1472\n",
      "Epoch [3/100] Batch 0/183                       Loss D: 0.1423, loss G: 2.7475\n",
      "Epoch [4/100] Batch 0/183                       Loss D: 0.7423, loss G: 1.3737\n",
      "Epoch [5/100] Batch 0/183                       Loss D: 0.6734, loss G: 0.9811\n",
      "Epoch [6/100] Batch 0/183                       Loss D: 0.2890, loss G: 1.5054\n",
      "Epoch [7/100] Batch 0/183                       Loss D: 0.8018, loss G: 0.6075\n",
      "Epoch [8/100] Batch 0/183                       Loss D: 0.4050, loss G: 1.0769\n",
      "Epoch [9/100] Batch 0/183                       Loss D: 0.7249, loss G: 0.6287\n",
      "Epoch [10/100] Batch 0/183                       Loss D: 0.4371, loss G: 1.1917\n",
      "Epoch [11/100] Batch 0/183                       Loss D: 0.4557, loss G: 1.2409\n",
      "Epoch [12/100] Batch 0/183                       Loss D: 0.6869, loss G: 0.8765\n",
      "Epoch [13/100] Batch 0/183                       Loss D: 0.5776, loss G: 1.0955\n",
      "Epoch [14/100] Batch 0/183                       Loss D: 0.4961, loss G: 1.1254\n",
      "Epoch [15/100] Batch 0/183                       Loss D: 0.6078, loss G: 0.8871\n",
      "Epoch [16/100] Batch 0/183                       Loss D: 0.4754, loss G: 1.0063\n",
      "Epoch [17/100] Batch 0/183                       Loss D: 1.2007, loss G: 0.4182\n",
      "Epoch [18/100] Batch 0/183                       Loss D: 0.4163, loss G: 1.2767\n",
      "Epoch [19/100] Batch 0/183                       Loss D: 0.5111, loss G: 1.0440\n",
      "Epoch [20/100] Batch 0/183                       Loss D: 0.4074, loss G: 1.1941\n",
      "Epoch [21/100] Batch 0/183                       Loss D: 0.6174, loss G: 0.9528\n",
      "Epoch [22/100] Batch 0/183                       Loss D: 0.9850, loss G: 0.6186\n",
      "Epoch [23/100] Batch 0/183                       Loss D: 0.6138, loss G: 0.8172\n",
      "Epoch [24/100] Batch 0/183                       Loss D: 0.8601, loss G: 0.6948\n",
      "Epoch [25/100] Batch 0/183                       Loss D: 0.6545, loss G: 0.9866\n",
      "Epoch [26/100] Batch 0/183                       Loss D: 0.4085, loss G: 1.4379\n",
      "Epoch [27/100] Batch 0/183                       Loss D: 0.6881, loss G: 0.9259\n",
      "Epoch [28/100] Batch 0/183                       Loss D: 0.7759, loss G: 0.7498\n",
      "Epoch [29/100] Batch 0/183                       Loss D: 0.5491, loss G: 0.9995\n",
      "Epoch [30/100] Batch 0/183                       Loss D: 0.8939, loss G: 0.5876\n",
      "Epoch [31/100] Batch 0/183                       Loss D: 0.5644, loss G: 0.9100\n",
      "Epoch [32/100] Batch 0/183                       Loss D: 0.5420, loss G: 0.8852\n",
      "Epoch [33/100] Batch 0/183                       Loss D: 0.5232, loss G: 0.9819\n",
      "Epoch [34/100] Batch 0/183                       Loss D: 0.7768, loss G: 0.6634\n",
      "Epoch [35/100] Batch 0/183                       Loss D: 0.7439, loss G: 0.7750\n",
      "Epoch [36/100] Batch 0/183                       Loss D: 0.5502, loss G: 1.0090\n",
      "Epoch [37/100] Batch 0/183                       Loss D: 0.9297, loss G: 0.6120\n",
      "Epoch [38/100] Batch 0/183                       Loss D: 0.5200, loss G: 1.1133\n",
      "Epoch [39/100] Batch 0/183                       Loss D: 0.7235, loss G: 0.6847\n",
      "Epoch [40/100] Batch 0/183                       Loss D: 0.6710, loss G: 1.0675\n",
      "Epoch [41/100] Batch 0/183                       Loss D: 0.6329, loss G: 0.7971\n",
      "Epoch [42/100] Batch 0/183                       Loss D: 1.1003, loss G: 0.4740\n",
      "Epoch [43/100] Batch 0/183                       Loss D: 0.7524, loss G: 0.8604\n",
      "Epoch [44/100] Batch 0/183                       Loss D: 0.6269, loss G: 0.8769\n",
      "Epoch [45/100] Batch 0/183                       Loss D: 0.6678, loss G: 0.7902\n",
      "Epoch [46/100] Batch 0/183                       Loss D: 0.5107, loss G: 1.1855\n",
      "Epoch [47/100] Batch 0/183                       Loss D: 0.7398, loss G: 0.7921\n",
      "Epoch [48/100] Batch 0/183                       Loss D: 0.6815, loss G: 0.8434\n",
      "Epoch [49/100] Batch 0/183                       Loss D: 0.5581, loss G: 0.8840\n",
      "Epoch [50/100] Batch 0/183                       Loss D: 0.8166, loss G: 0.9025\n",
      "Epoch [51/100] Batch 0/183                       Loss D: 0.6329, loss G: 0.7787\n",
      "Epoch [52/100] Batch 0/183                       Loss D: 0.7091, loss G: 0.6506\n",
      "Epoch [53/100] Batch 0/183                       Loss D: 0.8480, loss G: 0.6022\n",
      "Epoch [54/100] Batch 0/183                       Loss D: 0.6626, loss G: 0.7976\n",
      "Epoch [55/100] Batch 0/183                       Loss D: 0.7974, loss G: 0.7039\n",
      "Epoch [56/100] Batch 0/183                       Loss D: 0.6851, loss G: 0.9324\n",
      "Epoch [57/100] Batch 0/183                       Loss D: 1.2398, loss G: 0.5828\n",
      "Epoch [58/100] Batch 0/183                       Loss D: 0.8480, loss G: 0.7066\n",
      "Epoch [59/100] Batch 0/183                       Loss D: 0.4968, loss G: 1.2186\n",
      "Epoch [60/100] Batch 0/183                       Loss D: 0.8054, loss G: 0.8039\n",
      "Epoch [61/100] Batch 0/183                       Loss D: 0.8356, loss G: 0.5559\n",
      "Epoch [62/100] Batch 0/183                       Loss D: 1.2423, loss G: 0.4668\n",
      "Epoch [63/100] Batch 0/183                       Loss D: 0.8558, loss G: 0.7133\n",
      "Epoch [64/100] Batch 0/183                       Loss D: 0.7553, loss G: 0.7027\n",
      "Epoch [65/100] Batch 0/183                       Loss D: 0.6008, loss G: 0.7428\n",
      "Epoch [66/100] Batch 0/183                       Loss D: 1.0702, loss G: 0.4883\n",
      "Epoch [67/100] Batch 0/183                       Loss D: 0.5454, loss G: 1.2508\n",
      "Epoch [68/100] Batch 0/183                       Loss D: 0.7887, loss G: 0.5899\n",
      "Epoch [69/100] Batch 0/183                       Loss D: 0.5829, loss G: 0.8600\n",
      "Epoch [70/100] Batch 0/183                       Loss D: 0.8546, loss G: 0.7095\n",
      "Epoch [71/100] Batch 0/183                       Loss D: 0.5908, loss G: 0.9192\n",
      "Epoch [72/100] Batch 0/183                       Loss D: 1.3623, loss G: 0.3091\n",
      "Epoch [73/100] Batch 0/183                       Loss D: 0.6605, loss G: 1.0369\n",
      "Epoch [74/100] Batch 0/183                       Loss D: 0.9063, loss G: 0.5922\n",
      "Epoch [75/100] Batch 0/183                       Loss D: 0.6964, loss G: 0.7645\n",
      "Epoch [76/100] Batch 0/183                       Loss D: 0.7296, loss G: 0.7116\n",
      "Epoch [77/100] Batch 0/183                       Loss D: 0.9534, loss G: 0.5235\n",
      "Epoch [78/100] Batch 0/183                       Loss D: 0.7871, loss G: 0.5128\n",
      "Epoch [79/100] Batch 0/183                       Loss D: 0.7083, loss G: 0.6771\n",
      "Epoch [80/100] Batch 0/183                       Loss D: 0.9789, loss G: 0.5500\n",
      "Epoch [81/100] Batch 0/183                       Loss D: 0.8701, loss G: 0.6058\n",
      "Epoch [82/100] Batch 0/183                       Loss D: 0.8695, loss G: 0.6983\n",
      "Epoch [83/100] Batch 0/183                       Loss D: 0.6029, loss G: 0.7514\n",
      "Epoch [84/100] Batch 0/183                       Loss D: 0.6555, loss G: 0.9090\n",
      "Epoch [85/100] Batch 0/183                       Loss D: 1.1121, loss G: 0.5951\n",
      "Epoch [86/100] Batch 0/183                       Loss D: 0.8261, loss G: 0.6353\n",
      "Epoch [87/100] Batch 0/183                       Loss D: 0.5595, loss G: 1.0813\n",
      "Epoch [88/100] Batch 0/183                       Loss D: 0.7088, loss G: 0.7758\n",
      "Epoch [89/100] Batch 0/183                       Loss D: 0.7299, loss G: 0.9505\n",
      "Epoch [90/100] Batch 0/183                       Loss D: 0.8485, loss G: 0.6339\n",
      "Epoch [91/100] Batch 0/183                       Loss D: 0.6952, loss G: 0.7138\n",
      "Epoch [92/100] Batch 0/183                       Loss D: 0.6372, loss G: 1.1182\n",
      "Epoch [93/100] Batch 0/183                       Loss D: 0.6473, loss G: 0.9850\n",
      "Epoch [94/100] Batch 0/183                       Loss D: 0.5184, loss G: 0.9986\n",
      "Epoch [95/100] Batch 0/183                       Loss D: 0.7372, loss G: 0.7779\n",
      "Epoch [96/100] Batch 0/183                       Loss D: 0.7667, loss G: 0.6108\n",
      "Epoch [97/100] Batch 0/183                       Loss D: 0.9068, loss G: 0.5437\n",
      "Epoch [98/100] Batch 0/183                       Loss D: 0.7525, loss G: 0.6300\n",
      "Epoch [99/100] Batch 0/183                       Loss D: 0.6856, loss G: 0.8279\n"
     ]
    }
   ],
   "source": [
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer = SummaryWriter(f\"logs/{TENSORBOARD_MODEL_NAME}\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, real in enumerate(loader):\n",
    "        real = real.view(-1, image_dim).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "                data = real.reshape(-1, 1, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "                writer.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "\n",
    "                writer.add_scalar(\"data/loss_dis\", lossD.item(), global_step=step)\n",
    "                writer.add_scalar(\"data/loss_gen\", lossG.item(), global_step=step)\n",
    "\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2489d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
